{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b5f1457-08b0-47e8-b219-d563dd18e1ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "# Write a script to collect all JMX metrics available for all Java processes\n",
    "# into a JSON snippet compatible with the \"jmxMetrics\" field of the applicationinsights.json file\n",
    "# (Application Insights Java agent configuration file).\n",
    "# See https://learn.microsoft.com/azure/azure-monitor/app/java-jmx-metrics-configuration\n",
    "\n",
    "rm -rf /dbfs/metrics/\n",
    "mkdir -p /dbfs/metrics/result\n",
    "\n",
    "cat <<EOF >/dbfs/metrics/dump-jmx-metrics.sh\n",
    "\n",
    "test -e jmxterm-1.0.4-uber.jar || wget -q https://github.com/jiaqi/jmxterm/releases/download/v1.0.4/jmxterm-1.0.4-uber.jar\n",
    "for pid in \\$(ps -e | grep java | cut -f3 -d ' '); do\n",
    "\n",
    "  echo \"PID: \\$pid\"\n",
    "\n",
    "  # Output available domains to notebook\n",
    "  echo \"open \\$pid\" > jmxterm-commands.txt\n",
    "  echo \"domains\" >> jmxterm-commands.txt\n",
    "  java -jar jmxterm-1.0.4-uber.jar -i jmxterm-commands.txt\n",
    "\n",
    "  # List MBean names in desired domains\n",
    "  echo \"open \\$pid\" > jmxterm-commands.txt\n",
    "  for domain in \"java.lang\" \"metrics\"; do\n",
    "    echo \"beans -d \\$domain\" >> jmxterm-commands.txt\n",
    "  done\n",
    "  java -jar jmxterm-1.0.4-uber.jar -i jmxterm-commands.txt > jmxterm-output.txt\n",
    "\n",
    "  # Skip java processes that are not related to Spark\n",
    "  grep --quiet name=spark jmxterm-output.txt || continue\n",
    "\n",
    "  # Generate jmxterm commands file to list attributes for each MBean\n",
    "  echo \"open \\$pid\" > jmxterm-commands-detailed.txt\n",
    "  python -c 'import sys, re; [sys.stdout.write(f\"bean {line}\\ninfo\\n\") for line in sys.stdin]' < jmxterm-output.txt >> jmxterm-commands-detailed.txt\n",
    "  sed -i 's/ /\\\\\\\\ /2g' jmxterm-commands-detailed.txt\n",
    "  cat jmxterm-commands-detailed.txt\n",
    "  \n",
    "  # Output MBeans detailed information to output file\n",
    "  java -jar jmxterm-1.0.4-uber.jar -i jmxterm-commands-detailed.txt > /dbfs/metrics/result/\\$1\\$SPARK_LOCAL_IP-\\$pid.txt 2>&1\n",
    "done\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9807d3fe-e5f4-477b-8b23-dbac807161be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID: 488\nWelcome to JMX terminal. Type \"help\" for available commands.\n#Connection to 488 is opened\n#following domains are available\nJMImplementation\ncom.sun.management\njava.lang\njava.nio\njava.util.logging\njdk.management.jfr\norg.apache.logging.log4j2\nWelcome to JMX terminal. Type \"help\" for available commands.\n#Connection to 488 is opened\n#domain = java.lang:\n#IllegalArgumentException: Domain metrics doesn't exist, check your spelling\nPID: 587\nWelcome to JMX terminal. Type \"help\" for available commands.\n#Connection to 587 is opened\n#following domains are available\nJMImplementation\ncom.sun.management\njava.lang\njava.nio\njava.util.logging\njdk.management.jfr\norg.apache.logging.log4j2\nWelcome to JMX terminal. Type \"help\" for available commands.\n#Connection to 587 is opened\n#domain = java.lang:\n#IllegalArgumentException: Domain metrics doesn't exist, check your spelling\nPID: 813\nWelcome to JMX terminal. Type \"help\" for available commands.\n#Connection to 813 is opened\n#following domains are available\nJMImplementation\ncom.amazonaws.management\ncom.sun.management\njava.lang\njava.nio\njava.util.logging\njdk.management.jfr\nmetrics\norg.apache.commons.pool2\norg.apache.logging.log4j2\nWelcome to JMX terminal. Type \"help\" for available commands.\n#Connection to 813 is opened\n#domain = java.lang:\n#domain = metrics:\nopen 813\nbean java.lang:name=Code\\ Cache,type=MemoryPool\n\ninfo\nbean java.lang:name=CodeCacheManager,type=MemoryManager\n\ninfo\nbean java.lang:name=Compressed\\ Class\\ Space,type=MemoryPool\n\ninfo\nbean java.lang:name=Metaspace\\ Manager,type=MemoryManager\n\ninfo\nbean java.lang:name=Metaspace,type=MemoryPool\n\ninfo\nbean java.lang:name=PS\\ Eden\\ Space,type=MemoryPool\n\ninfo\nbean java.lang:name=PS\\ MarkSweep,type=GarbageCollector\n\ninfo\nbean java.lang:name=PS\\ Old\\ Gen,type=MemoryPool\n\ninfo\nbean java.lang:name=PS\\ Scavenge,type=GarbageCollector\n\ninfo\nbean java.lang:name=PS\\ Survivor\\ Space,type=MemoryPool\n\ninfo\nbean java.lang:type=ClassLoading\n\ninfo\nbean java.lang:type=Compilation\n\ninfo\nbean java.lang:type=Memory\n\ninfo\nbean java.lang:type=OperatingSystem\n\ninfo\nbean java.lang:type=Runtime\n\ninfo\nbean java.lang:type=Threading\n\ninfo\nbean metrics:name=spark.driver.BlockManager.memory.diskSpaceUsed_MB,type=gauges\n\ninfo\nbean metrics:name=spark.driver.BlockManager.memory.maxMem_MB,type=gauges\n\ninfo\nbean metrics:name=spark.driver.BlockManager.memory.maxOffHeapMem_MB,type=gauges\n\ninfo\nbean metrics:name=spark.driver.BlockManager.memory.maxOnHeapMem_MB,type=gauges\n\ninfo\nbean metrics:name=spark.driver.BlockManager.memory.memUsed_MB,type=gauges\n\ninfo\nbean metrics:name=spark.driver.BlockManager.memory.offHeapMemUsed_MB,type=gauges\n\ninfo\nbean metrics:name=spark.driver.BlockManager.memory.onHeapMemUsed_MB,type=gauges\n\ninfo\nbean metrics:name=spark.driver.BlockManager.memory.remainingMem_MB,type=gauges\n\ninfo\nbean metrics:name=spark.driver.BlockManager.memory.remainingOffHeapMem_MB,type=gauges\n\ninfo\nbean metrics:name=spark.driver.BlockManager.memory.remainingOnHeapMem_MB,type=gauges\n\ninfo\nbean metrics:name=spark.driver.CodeGenerator.compilationTime,type=histograms\n\ninfo\nbean metrics:name=spark.driver.CodeGenerator.generatedClassSize,type=histograms\n\ninfo\nbean metrics:name=spark.driver.CodeGenerator.generatedMethodSize,type=histograms\n\ninfo\nbean metrics:name=spark.driver.CodeGenerator.sourceCodeSize,type=histograms\n\ninfo\nbean metrics:name=spark.driver.DAGScheduler.job.activeJobs,type=gauges\n\ninfo\nbean metrics:name=spark.driver.DAGScheduler.job.allJobs,type=gauges\n\ninfo\nbean metrics:name=spark.driver.DAGScheduler.messageProcessingTime,type=timers\n\ninfo\nbean metrics:name=spark.driver.DAGScheduler.stage.failedStages,type=gauges\n\ninfo\nbean metrics:name=spark.driver.DAGScheduler.stage.runningStages,type=gauges\n\ninfo\nbean metrics:name=spark.driver.DAGScheduler.stage.waitingStages,type=gauges\n\ninfo\nbean metrics:name=spark.driver.Databricks.directoryCommit.autoVacuumCount,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.directoryCommit.deletedFilesFiltered,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.directoryCommit.filterListingCount,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.directoryCommit.jobCommitCompleted,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.directoryCommit.markerReadErrors,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.directoryCommit.markerRefreshCount,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.directoryCommit.markerRefreshErrors,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.directoryCommit.markersRead,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.directoryCommit.repeatedListCount,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.directoryCommit.uncommittedFilesFiltered,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.directoryCommit.untrackedFilesFound,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.directoryCommit.vacuumCount,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.directoryCommit.vacuumErrors,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.preemption.numChecks,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.preemption.numPoolsAutoExpired,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.preemption.numTasksPreempted,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.preemption.poolStarvationMillis,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.preemption.schedulerOverheadNanos,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.preemption.taskTimeWastedMillis,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.taskSchedulingLanes.activePools,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.taskSchedulingLanes.bypassLaneActivePools,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.taskSchedulingLanes.fastLaneActivePools,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.taskSchedulingLanes.finishedQueriesTotalTaskTimeNs,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.taskSchedulingLanes.laneCleanup.markedPools,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.taskSchedulingLanes.laneCleanup.twoPhasePoolsCleaned,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.taskSchedulingLanes.laneCleanup.zombiePoolsCleaned,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.taskSchedulingLanes.preemption.slotTransferNumSuccessfulPreemptionIterations,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.taskSchedulingLanes.preemption.slotTransferNumTasksPreempted,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.taskSchedulingLanes.preemption.slotTransferWastedTaskTimeNs,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.taskSchedulingLanes.slotReservation.numGradualDecrease,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.taskSchedulingLanes.slotReservation.numQuickDrop,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.taskSchedulingLanes.slotReservation.numQuickJump,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.taskSchedulingLanes.slotReservation.slotsReserved,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.taskSchedulingLanes.slowLaneActivePools,type=counters\n\ninfo\nbean metrics:name=spark.driver.Databricks.taskSchedulingLanes.totalQueryGroupsFinished,type=counters\n\ninfo\nbean metrics:name=spark.driver.ExecutorMetrics.DirectPoolMemory,type=gauges\n\ninfo\nbean metrics:name=spark.driver.ExecutorMetrics.JVMHeapMemory,type=gauges\n\ninfo\nbean metrics:name=spark.driver.ExecutorMetrics.JVMOffHeapMemory,type=gauges\n\ninfo\nbean metrics:name=spark.driver.ExecutorMetrics.MajorGCCount,type=gauges\n\ninfo\nbean metrics:name=spark.driver.ExecutorMetrics.MajorGCTime,type=gauges\n\ninfo\nbean metrics:name=spark.driver.ExecutorMetrics.MappedPoolMemory,type=gauges\n\ninfo\nbean metrics:name=spark.driver.ExecutorMetrics.MinorGCCount,type=gauges\n\ninfo\nbean metrics:name=spark.driver.ExecutorMetrics.MinorGCTime,type=gauges\n\ninfo\nbean metrics:name=spark.driver.ExecutorMetrics.OffHeapExecutionMemory,type=gauges\n\ninfo\nbean metrics:name=spark.driver.ExecutorMetrics.OffHeapStorageMemory,type=gauges\n\ninfo\nbean metrics:name=spark.driver.ExecutorMetrics.OffHeapUnifiedMemory,type=gauges\n\ninfo\nbean metrics:name=spark.driver.ExecutorMetrics.OnHeapExecutionMemory,type=gauges\n\ninfo\nbean metrics:name=spark.driver.ExecutorMetrics.OnHeapStorageMemory,type=gauges\n\ninfo\nbean metrics:name=spark.driver.ExecutorMetrics.OnHeapUnifiedMemory,type=gauges\n\ninfo\nbean metrics:name=spark.driver.ExecutorMetrics.ProcessTreeJVMRSSMemory,type=gauges\n\ninfo\nbean metrics:name=spark.driver.ExecutorMetrics.ProcessTreeJVMVMemory,type=gauges\n\ninfo\nbean metrics:name=spark.driver.ExecutorMetrics.ProcessTreeOtherRSSMemory,type=gauges\n\ninfo\nbean metrics:name=spark.driver.ExecutorMetrics.ProcessTreeOtherVMemory,type=gauges\n\ninfo\nbean metrics:name=spark.driver.ExecutorMetrics.ProcessTreePythonRSSMemory,type=gauges\n\ninfo\nbean metrics:name=spark.driver.ExecutorMetrics.ProcessTreePythonVMemory,type=gauges\n\ninfo\nbean metrics:name=spark.driver.ExecutorMetrics.TotalGCTime,type=gauges\n\ninfo\nbean metrics:name=spark.driver.HiveExternalCatalog.fileCacheHits,type=counters\n\ninfo\nbean metrics:name=spark.driver.HiveExternalCatalog.filesDiscovered,type=counters\n\ninfo\nbean metrics:name=spark.driver.HiveExternalCatalog.hiveClientCalls,type=counters\n\ninfo\nbean metrics:name=spark.driver.HiveExternalCatalog.parallelListingJobCount,type=counters\n\ninfo\nbean metrics:name=spark.driver.HiveExternalCatalog.partitionsFetched,type=counters\n\ninfo\nbean metrics:name=spark.driver.JVMCPU.jvmCpuTime,type=gauges\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.listenerProcessingTime.com.databricks.backend.daemon.driver.DBCEventLoggingListener,type=timers\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.listenerProcessingTime.com.databricks.backend.daemon.driver.DataPlaneEventListener,type=timers\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.listenerProcessingTime.com.databricks.backend.daemon.driver.NephosColdQueryListener,type=timers\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.listenerProcessingTime.com.databricks.photon.PhotonCleanupListener,type=timers\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.listenerProcessingTime.com.databricks.spark.sql.execution.SparkEBJCleanupListener,type=timers\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.listenerProcessingTime.com.databricks.spark.util.ExecutorTimeLoggingListener$,type=timers\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.listenerProcessingTime.com.databricks.spark.util.UsageLoggingListener,type=timers\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.listenerProcessingTime.com.databricks.sql.advice.AdvisorListener,type=timers\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.listenerProcessingTime.com.databricks.sql.debugger.QueryWatchdogListener,type=timers\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.listenerProcessingTime.com.databricks.sql.execution.ui.IOCacheListener,type=timers\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.listenerProcessingTime.com.databricks.sql.io.caching.RepeatedReadsEstimator$,type=timers\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.listenerProcessingTime.com.databricks.sql.logging.QueryProfileListener,type=timers\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.listenerProcessingTime.org.apache.spark.sql.QueryProfileListener,type=timers\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.listenerProcessingTime.org.apache.spark.sql.SparkSession$$anon$1,type=timers\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.listenerProcessingTime.org.apache.spark.sql.execution.SQLExecution$,type=timers\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.listenerProcessingTime.org.apache.spark.sql.execution.streaming.StreamingQueryListenerBus,type=timers\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.listenerProcessingTime.org.apache.spark.sql.execution.ui.SQLAppStatusListener,type=timers\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.listenerProcessingTime.org.apache.spark.sql.hive.thriftserver.ui.HiveThriftServer2Listener,type=timers\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.listenerProcessingTime.org.apache.spark.status.AppStatusListener,type=timers\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.listenerProcessingTime.org.apache.spark.util.ProfilerEnv$$anon$1,type=timers\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.numEventsPosted,type=counters\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.queue.appStatus.listenerProcessingTime,type=timers\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.queue.appStatus.numDroppedEvents,type=counters\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.queue.appStatus.size,type=gauges\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.queue.executorManagement.listenerProcessingTime,type=timers\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.queue.executorManagement.numDroppedEvents,type=counters\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.queue.executorManagement.size,type=gauges\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.queue.query-profile-logging-queue.listenerProcessingTime,type=timers\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.queue.query-profile-logging-queue.numDroppedEvents,type=counters\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.queue.query-profile-logging-queue.size,type=gauges\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.queue.shared.listenerProcessingTime,type=timers\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.queue.shared.numDroppedEvents,type=counters\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.queue.shared.size,type=gauges\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.queue.streams.listenerProcessingTime,type=timers\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.queue.streams.numDroppedEvents,type=counters\n\ninfo\nbean metrics:name=spark.driver.LiveListenerBus.queue.streams.size,type=gauges\n\ninfo\nbean metrics:name=spark.driver.SparkSQLOperationManager.numHiveOperations,type=gauges\n\ninfo\nbean metrics:name=spark.driver.SparkStatusTracker.numInconsistentGetJobInfoResponses,type=counters\n\ninfo\nbean metrics:name=spark.driver.appStatus.jobDuration,type=gauges\n\ninfo\nbean metrics:name=spark.driver.appStatus.jobs.failedJobs,type=counters\n\ninfo\nbean metrics:name=spark.driver.appStatus.jobs.succeededJobs,type=counters\n\ninfo\nbean metrics:name=spark.driver.appStatus.stages.completedStages,type=counters\n\ninfo\nbean metrics:name=spark.driver.appStatus.stages.failedStages,type=counters\n\ninfo\nbean metrics:name=spark.driver.appStatus.stages.skippedStages,type=counters\n\ninfo\nbean metrics:name=spark.driver.appStatus.tasks.blackListedExecutors,type=counters\n\ninfo\nbean metrics:name=spark.driver.appStatus.tasks.completedTasks,type=counters\n\ninfo\nbean metrics:name=spark.driver.appStatus.tasks.excludedExecutors,type=counters\n\ninfo\nbean metrics:name=spark.driver.appStatus.tasks.failedTasks,type=counters\n\ninfo\nbean metrics:name=spark.driver.appStatus.tasks.killedTasks,type=counters\n\ninfo\nbean metrics:name=spark.driver.appStatus.tasks.skippedTasks,type=counters\n\ninfo\nbean metrics:name=spark.driver.appStatus.tasks.unblackListedExecutors,type=counters\n\ninfo\nbean metrics:name=spark.driver.appStatus.tasks.unexcludedExecutors,type=counters\n\ninfo\n"
     ]
    }
   ],
   "source": [
    "%sh \n",
    "# Run the script for the driver\n",
    "bash /dbfs/metrics/dump-jmx-metrics.sh driver-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dde58ef-6f99-4984-a59c-6249cf5549d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">import scala.concurrent.duration._\n",
       "res: scala.collection.Map[String,scala.util.Try[String]] =\n",
       "Map(0 -&gt; Success(PID: 646\n",
       "JMImplementation\n",
       "com.sun.management\n",
       "java.lang\n",
       "java.nio\n",
       "java.util.logging\n",
       "jdk.management.jfr\n",
       "org.apache.logging.log4j2\n",
       "PID: 833\n",
       "JMImplementation\n",
       "com.amazonaws.management\n",
       "com.sun.management\n",
       "java.lang\n",
       "java.nio\n",
       "java.util.logging\n",
       "jdk.management.jfr\n",
       "metrics\n",
       "org.apache.logging.log4j2\n",
       "open 833\n",
       "bean java.lang:name=Code\\ Cache,type=MemoryPool\n",
       "\n",
       "info\n",
       "bean java.lang:name=CodeCacheManager,type=MemoryManager\n",
       "\n",
       "info\n",
       "bean java.lang:name=Compressed\\ Class\\ Space,type=MemoryPool\n",
       "\n",
       "info\n",
       "bean java.lang:name=Metaspace\\ Manager,type=MemoryManager\n",
       "\n",
       "info\n",
       "bean java.lang:name=Metaspace,type=MemoryPool\n",
       "\n",
       "info\n",
       "bean java.lang:name=PS\\ Eden\\ Space,type=MemoryPool\n",
       "\n",
       "info\n",
       "bean java.lang:name=PS\\ MarkSweep,type=GarbageCollector\n",
       "\n",
       "info\n",
       "bean java.lang:name=PS\\ Old\\ Gen,type=MemoryPool\n",
       "\n",
       "info\n",
       "bean java.lang:name=PS\\ Scavenge,type=GarbageCollector\n",
       "\n",
       "info\n",
       "bean java.lang:name=PS\\ Survivor\\ Space,type=MemoryPool\n",
       "\n",
       "info\n",
       "bean java.lang:type=ClassLoading\n",
       "\n",
       "info\n",
       "bean java.lang:type=Compilation\n",
       "\n",
       "info\n",
       "bean java.lang:type=Memory\n",
       "\n",
       "info\n",
       "bean java.lang:type=OperatingSystem\n",
       "\n",
       "info\n",
       "bean java.lang:type=Runtime\n",
       "\n",
       "info\n",
       "bean java.lang:type=Threading\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.CodeGenerator.compilationTime,type=histograms\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.CodeGenerator.generatedClassSize,type=histograms\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.CodeGenerator.generatedMethodSize,type=histograms\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.CodeGenerator.sourceCodeSize,type=histograms\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.directoryCommit.autoVacuumCount,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.directoryCommit.deletedFilesFiltered,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.directoryCommit.filterListingCount,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.directoryCommit.jobCommitCompleted,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.directoryCommit.markerReadErrors,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.directoryCommit.markerRefreshCount,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.directoryCommit.markerRefreshErrors,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.directoryCommit.markersRead,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.directoryCommit.repeatedListCount,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.directoryCommit.uncommittedFilesFiltered,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.directoryCommit.untrackedFilesFound,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.directoryCommit.vacuumCount,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.directoryCommit.vacuumErrors,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.preemption.numChecks,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.preemption.numPoolsAutoExpired,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.preemption.numTasksPreempted,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.preemption.poolStarvationMillis,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.preemption.schedulerOverheadNanos,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.preemption.taskTimeWastedMillis,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.taskSchedulingLanes.activePools,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.taskSchedulingLanes.bypassLaneActivePools,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.taskSchedulingLanes.fastLaneActivePools,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.taskSchedulingLanes.finishedQueriesTotalTaskTimeNs,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.taskSchedulingLanes.laneCleanup.markedPools,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.taskSchedulingLanes.laneCleanup.twoPhasePoolsCleaned,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.taskSchedulingLanes.laneCleanup.zombiePoolsCleaned,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.taskSchedulingLanes.preemption.slotTransferNumSuccessfulPreemptionIterations,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.taskSchedulingLanes.preemption.slotTransferNumTasksPreempted,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.taskSchedulingLanes.preemption.slotTransferWastedTaskTimeNs,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.taskSchedulingLanes.slotReservation.numGradualDecrease,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.taskSchedulingLanes.slotReservation.numQuickDrop,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.taskSchedulingLanes.slotReservation.numQuickJump,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.taskSchedulingLanes.slotReservation.slotsReserved,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.taskSchedulingLanes.slowLaneActivePools,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.Databricks.taskSchedulingLanes.totalQueryGroupsFinished,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.ExecutorMetrics.DirectPoolMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.ExecutorMetrics.JVMHeapMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.ExecutorMetrics.JVMOffHeapMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.ExecutorMetrics.MajorGCCount,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.ExecutorMetrics.MajorGCTime,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.ExecutorMetrics.MappedPoolMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.ExecutorMetrics.MinorGCCount,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.ExecutorMetrics.MinorGCTime,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.ExecutorMetrics.OffHeapExecutionMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.ExecutorMetrics.OffHeapStorageMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.ExecutorMetrics.OffHeapUnifiedMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.ExecutorMetrics.OnHeapExecutionMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.ExecutorMetrics.OnHeapStorageMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.ExecutorMetrics.OnHeapUnifiedMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.ExecutorMetrics.ProcessTreeJVMRSSMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.ExecutorMetrics.ProcessTreeJVMVMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.ExecutorMetrics.ProcessTreeOtherRSSMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.ExecutorMetrics.ProcessTreeOtherVMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.ExecutorMetrics.ProcessTreePythonRSSMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.ExecutorMetrics.ProcessTreePythonVMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.ExecutorMetrics.TotalGCTime,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.ExternalShuffle.shuffle-client.usedDirectMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.ExternalShuffle.shuffle-client.usedHeapMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.HiveExternalCatalog.fileCacheHits,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.HiveExternalCatalog.filesDiscovered,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.HiveExternalCatalog.hiveClientCalls,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.HiveExternalCatalog.parallelListingJobCount,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.HiveExternalCatalog.partitionsFetched,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.JVMCPU.jvmCpuTime,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.bytesRead,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.bytesWritten,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.cpuTime,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.deserializeCpuTime,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.deserializeTime,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.diskBytesSpilled,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.filesystem.file.largeRead_ops,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.filesystem.file.read_bytes,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.filesystem.file.read_ops,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.filesystem.file.write_bytes,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.filesystem.file.write_ops,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.filesystem.hdfs.largeRead_ops,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.filesystem.hdfs.read_bytes,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.filesystem.hdfs.read_ops,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.filesystem.hdfs.write_bytes,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.filesystem.hdfs.write_ops,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.jvmGCTime,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.memoryBytesSpilled,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.recordsRead,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.recordsWritten,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.resultSerializationTime,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.resultSize,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.runTime,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.shuffleBytesWritten,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.shuffleFetchWaitTime,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.shuffleLocalBlocksFetched,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.shuffleLocalBytesRead,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.shuffleRecordsRead,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.shuffleRecordsWritten,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.shuffleRemoteBlocksFetched,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.shuffleRemoteBytesRead,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.shuffleRemoteBytesReadToDisk,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.shuffleTotalBytesRead,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.shuffleWriteTime,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.succeededTasks,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.threadpool.activeTasks,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.threadpool.completeTasks,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.threadpool.currentPool_size,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.threadpool.maxPool_size,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.0.executor.threadpool.startedTasks,type=gauges\n",
       "\n",
       "info\n",
       "), 1 -&gt; Success(PID: 646\n",
       "JMImplementation\n",
       "com.sun.management\n",
       "java.lang\n",
       "java.nio\n",
       "java.util.logging\n",
       "jdk.management.jfr\n",
       "org.apache.logging.log4j2\n",
       "PID: 833\n",
       "JMImplementation\n",
       "com.amazonaws.management\n",
       "com.sun.management\n",
       "java.lang\n",
       "java.nio\n",
       "java.util.logging\n",
       "jdk.management.jfr\n",
       "metrics\n",
       "org.apache.logging.log4j2\n",
       "open 833\n",
       "bean java.lang:name=Code\\ Cache,type=MemoryPool\n",
       "\n",
       "info\n",
       "bean java.lang:name=CodeCacheManager,type=MemoryManager\n",
       "\n",
       "info\n",
       "bean java.lang:name=Compressed\\ Class\\ Space,type=MemoryPool\n",
       "\n",
       "info\n",
       "bean java.lang:name=Metaspace\\ Manager,type=MemoryManager\n",
       "\n",
       "info\n",
       "bean java.lang:name=Metaspace,type=MemoryPool\n",
       "\n",
       "info\n",
       "bean java.lang:name=PS\\ Eden\\ Space,type=MemoryPool\n",
       "\n",
       "info\n",
       "bean java.lang:name=PS\\ MarkSweep,type=GarbageCollector\n",
       "\n",
       "info\n",
       "bean java.lang:name=PS\\ Old\\ Gen,type=MemoryPool\n",
       "\n",
       "info\n",
       "bean java.lang:name=PS\\ Scavenge,type=GarbageCollector\n",
       "\n",
       "info\n",
       "bean java.lang:name=PS\\ Survivor\\ Space,type=MemoryPool\n",
       "\n",
       "info\n",
       "bean java.lang:type=ClassLoading\n",
       "\n",
       "info\n",
       "bean java.lang:type=Compilation\n",
       "\n",
       "info\n",
       "bean java.lang:type=Memory\n",
       "\n",
       "info\n",
       "bean java.lang:type=OperatingSystem\n",
       "\n",
       "info\n",
       "bean java.lang:type=Runtime\n",
       "\n",
       "info\n",
       "bean java.lang:type=Threading\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.CodeGenerator.compilationTime,type=histograms\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.CodeGenerator.generatedClassSize,type=histograms\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.CodeGenerator.generatedMethodSize,type=histograms\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.CodeGenerator.sourceCodeSize,type=histograms\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.directoryCommit.autoVacuumCount,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.directoryCommit.deletedFilesFiltered,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.directoryCommit.filterListingCount,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.directoryCommit.jobCommitCompleted,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.directoryCommit.markerReadErrors,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.directoryCommit.markerRefreshCount,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.directoryCommit.markerRefreshErrors,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.directoryCommit.markersRead,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.directoryCommit.repeatedListCount,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.directoryCommit.uncommittedFilesFiltered,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.directoryCommit.untrackedFilesFound,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.directoryCommit.vacuumCount,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.directoryCommit.vacuumErrors,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.preemption.numChecks,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.preemption.numPoolsAutoExpired,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.preemption.numTasksPreempted,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.preemption.poolStarvationMillis,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.preemption.schedulerOverheadNanos,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.preemption.taskTimeWastedMillis,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.taskSchedulingLanes.activePools,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.taskSchedulingLanes.bypassLaneActivePools,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.taskSchedulingLanes.fastLaneActivePools,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.taskSchedulingLanes.finishedQueriesTotalTaskTimeNs,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.taskSchedulingLanes.laneCleanup.markedPools,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.taskSchedulingLanes.laneCleanup.twoPhasePoolsCleaned,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.taskSchedulingLanes.laneCleanup.zombiePoolsCleaned,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.taskSchedulingLanes.preemption.slotTransferNumSuccessfulPreemptionIterations,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.taskSchedulingLanes.preemption.slotTransferNumTasksPreempted,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.taskSchedulingLanes.preemption.slotTransferWastedTaskTimeNs,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.taskSchedulingLanes.slotReservation.numGradualDecrease,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.taskSchedulingLanes.slotReservation.numQuickDrop,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.taskSchedulingLanes.slotReservation.numQuickJump,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.taskSchedulingLanes.slotReservation.slotsReserved,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.taskSchedulingLanes.slowLaneActivePools,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.Databricks.taskSchedulingLanes.totalQueryGroupsFinished,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.ExecutorMetrics.DirectPoolMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.ExecutorMetrics.JVMHeapMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.ExecutorMetrics.JVMOffHeapMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.ExecutorMetrics.MajorGCCount,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.ExecutorMetrics.MajorGCTime,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.ExecutorMetrics.MappedPoolMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.ExecutorMetrics.MinorGCCount,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.ExecutorMetrics.MinorGCTime,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.ExecutorMetrics.OffHeapExecutionMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.ExecutorMetrics.OffHeapStorageMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.ExecutorMetrics.OffHeapUnifiedMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.ExecutorMetrics.OnHeapExecutionMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.ExecutorMetrics.OnHeapStorageMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.ExecutorMetrics.OnHeapUnifiedMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.ExecutorMetrics.ProcessTreeJVMRSSMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.ExecutorMetrics.ProcessTreeJVMVMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.ExecutorMetrics.ProcessTreeOtherRSSMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.ExecutorMetrics.ProcessTreeOtherVMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.ExecutorMetrics.ProcessTreePythonRSSMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.ExecutorMetrics.ProcessTreePythonVMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.ExecutorMetrics.TotalGCTime,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.ExternalShuffle.shuffle-client.usedDirectMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.ExternalShuffle.shuffle-client.usedHeapMemory,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.HiveExternalCatalog.fileCacheHits,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.HiveExternalCatalog.filesDiscovered,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.HiveExternalCatalog.hiveClientCalls,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.HiveExternalCatalog.parallelListingJobCount,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.HiveExternalCatalog.partitionsFetched,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.JVMCPU.jvmCpuTime,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.bytesRead,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.bytesWritten,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.cpuTime,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.deserializeCpuTime,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.deserializeTime,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.diskBytesSpilled,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.filesystem.file.largeRead_ops,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.filesystem.file.read_bytes,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.filesystem.file.read_ops,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.filesystem.file.write_bytes,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.filesystem.file.write_ops,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.filesystem.hdfs.largeRead_ops,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.filesystem.hdfs.read_bytes,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.filesystem.hdfs.read_ops,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.filesystem.hdfs.write_bytes,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.filesystem.hdfs.write_ops,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.jvmGCTime,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.memoryBytesSpilled,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.recordsRead,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.recordsWritten,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.resultSerializationTime,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.resultSize,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.runTime,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.shuffleBytesWritten,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.shuffleFetchWaitTime,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.shuffleLocalBlocksFetched,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.shuffleLocalBytesRead,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.shuffleRecordsRead,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.shuffleRecordsWritten,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.shuffleRemoteBlocksFetched,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.shuffleRemoteBytesRead,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.shuffleRemoteBytesReadToDisk,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.shuffleTotalBytesRead,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.shuffleWriteTime,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.succeededTasks,type=counters\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.threadpool.activeTasks,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.threadpool.completeTasks,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.threadpool.currentPool_size,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.threadpool.maxPool_size,type=gauges\n",
       "\n",
       "info\n",
       "bean metrics:name=spark.1.executor.threadpool.startedTasks,type=gauges\n",
       "\n",
       "info\n",
       "))\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">import scala.concurrent.duration._\nres: scala.collection.Map[String,scala.util.Try[String]] =\nMap(0 -&gt; Success(PID: 646\nJMImplementation\ncom.sun.management\njava.lang\njava.nio\njava.util.logging\njdk.management.jfr\norg.apache.logging.log4j2\nPID: 833\nJMImplementation\ncom.amazonaws.management\ncom.sun.management\njava.lang\njava.nio\njava.util.logging\njdk.management.jfr\nmetrics\norg.apache.logging.log4j2\nopen 833\nbean java.lang:name=Code\\ Cache,type=MemoryPool\n\ninfo\nbean java.lang:name=CodeCacheManager,type=MemoryManager\n\ninfo\nbean java.lang:name=Compressed\\ Class\\ Space,type=MemoryPool\n\ninfo\nbean java.lang:name=Metaspace\\ Manager,type=MemoryManager\n\ninfo\nbean java.lang:name=Metaspace,type=MemoryPool\n\ninfo\nbean java.lang:name=PS\\ Eden\\ Space,type=MemoryPool\n\ninfo\nbean java.lang:name=PS\\ MarkSweep,type=GarbageCollector\n\ninfo\nbean java.lang:name=PS\\ Old\\ Gen,type=MemoryPool\n\ninfo\nbean java.lang:name=PS\\ Scavenge,type=GarbageCollector\n\ninfo\nbean java.lang:name=PS\\ Survivor\\ Space,type=MemoryPool\n\ninfo\nbean java.lang:type=ClassLoading\n\ninfo\nbean java.lang:type=Compilation\n\ninfo\nbean java.lang:type=Memory\n\ninfo\nbean java.lang:type=OperatingSystem\n\ninfo\nbean java.lang:type=Runtime\n\ninfo\nbean java.lang:type=Threading\n\ninfo\nbean metrics:name=spark.0.CodeGenerator.compilationTime,type=histograms\n\ninfo\nbean metrics:name=spark.0.CodeGenerator.generatedClassSize,type=histograms\n\ninfo\nbean metrics:name=spark.0.CodeGenerator.generatedMethodSize,type=histograms\n\ninfo\nbean metrics:name=spark.0.CodeGenerator.sourceCodeSize,type=histograms\n\ninfo\nbean metrics:name=spark.0.Databricks.directoryCommit.autoVacuumCount,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.directoryCommit.deletedFilesFiltered,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.directoryCommit.filterListingCount,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.directoryCommit.jobCommitCompleted,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.directoryCommit.markerReadErrors,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.directoryCommit.markerRefreshCount,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.directoryCommit.markerRefreshErrors,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.directoryCommit.markersRead,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.directoryCommit.repeatedListCount,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.directoryCommit.uncommittedFilesFiltered,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.directoryCommit.untrackedFilesFound,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.directoryCommit.vacuumCount,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.directoryCommit.vacuumErrors,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.preemption.numChecks,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.preemption.numPoolsAutoExpired,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.preemption.numTasksPreempted,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.preemption.poolStarvationMillis,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.preemption.schedulerOverheadNanos,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.preemption.taskTimeWastedMillis,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.taskSchedulingLanes.activePools,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.taskSchedulingLanes.bypassLaneActivePools,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.taskSchedulingLanes.fastLaneActivePools,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.taskSchedulingLanes.finishedQueriesTotalTaskTimeNs,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.taskSchedulingLanes.laneCleanup.markedPools,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.taskSchedulingLanes.laneCleanup.twoPhasePoolsCleaned,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.taskSchedulingLanes.laneCleanup.zombiePoolsCleaned,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.taskSchedulingLanes.preemption.slotTransferNumSuccessfulPreemptionIterations,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.taskSchedulingLanes.preemption.slotTransferNumTasksPreempted,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.taskSchedulingLanes.preemption.slotTransferWastedTaskTimeNs,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.taskSchedulingLanes.slotReservation.numGradualDecrease,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.taskSchedulingLanes.slotReservation.numQuickDrop,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.taskSchedulingLanes.slotReservation.numQuickJump,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.taskSchedulingLanes.slotReservation.slotsReserved,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.taskSchedulingLanes.slowLaneActivePools,type=counters\n\ninfo\nbean metrics:name=spark.0.Databricks.taskSchedulingLanes.totalQueryGroupsFinished,type=counters\n\ninfo\nbean metrics:name=spark.0.ExecutorMetrics.DirectPoolMemory,type=gauges\n\ninfo\nbean metrics:name=spark.0.ExecutorMetrics.JVMHeapMemory,type=gauges\n\ninfo\nbean metrics:name=spark.0.ExecutorMetrics.JVMOffHeapMemory,type=gauges\n\ninfo\nbean metrics:name=spark.0.ExecutorMetrics.MajorGCCount,type=gauges\n\ninfo\nbean metrics:name=spark.0.ExecutorMetrics.MajorGCTime,type=gauges\n\ninfo\nbean metrics:name=spark.0.ExecutorMetrics.MappedPoolMemory,type=gauges\n\ninfo\nbean metrics:name=spark.0.ExecutorMetrics.MinorGCCount,type=gauges\n\ninfo\nbean metrics:name=spark.0.ExecutorMetrics.MinorGCTime,type=gauges\n\ninfo\nbean metrics:name=spark.0.ExecutorMetrics.OffHeapExecutionMemory,type=gauges\n\ninfo\nbean metrics:name=spark.0.ExecutorMetrics.OffHeapStorageMemory,type=gauges\n\ninfo\nbean metrics:name=spark.0.ExecutorMetrics.OffHeapUnifiedMemory,type=gauges\n\ninfo\nbean metrics:name=spark.0.ExecutorMetrics.OnHeapExecutionMemory,type=gauges\n\ninfo\nbean metrics:name=spark.0.ExecutorMetrics.OnHeapStorageMemory,type=gauges\n\ninfo\nbean metrics:name=spark.0.ExecutorMetrics.OnHeapUnifiedMemory,type=gauges\n\ninfo\nbean metrics:name=spark.0.ExecutorMetrics.ProcessTreeJVMRSSMemory,type=gauges\n\ninfo\nbean metrics:name=spark.0.ExecutorMetrics.ProcessTreeJVMVMemory,type=gauges\n\ninfo\nbean metrics:name=spark.0.ExecutorMetrics.ProcessTreeOtherRSSMemory,type=gauges\n\ninfo\nbean metrics:name=spark.0.ExecutorMetrics.ProcessTreeOtherVMemory,type=gauges\n\ninfo\nbean metrics:name=spark.0.ExecutorMetrics.ProcessTreePythonRSSMemory,type=gauges\n\ninfo\nbean metrics:name=spark.0.ExecutorMetrics.ProcessTreePythonVMemory,type=gauges\n\ninfo\nbean metrics:name=spark.0.ExecutorMetrics.TotalGCTime,type=gauges\n\ninfo\nbean metrics:name=spark.0.ExternalShuffle.shuffle-client.usedDirectMemory,type=gauges\n\ninfo\nbean metrics:name=spark.0.ExternalShuffle.shuffle-client.usedHeapMemory,type=gauges\n\ninfo\nbean metrics:name=spark.0.HiveExternalCatalog.fileCacheHits,type=counters\n\ninfo\nbean metrics:name=spark.0.HiveExternalCatalog.filesDiscovered,type=counters\n\ninfo\nbean metrics:name=spark.0.HiveExternalCatalog.hiveClientCalls,type=counters\n\ninfo\nbean metrics:name=spark.0.HiveExternalCatalog.parallelListingJobCount,type=counters\n\ninfo\nbean metrics:name=spark.0.HiveExternalCatalog.partitionsFetched,type=counters\n\ninfo\nbean metrics:name=spark.0.JVMCPU.jvmCpuTime,type=gauges\n\ninfo\nbean metrics:name=spark.0.executor.bytesRead,type=counters\n\ninfo\nbean metrics:name=spark.0.executor.bytesWritten,type=counters\n\ninfo\nbean metrics:name=spark.0.executor.cpuTime,type=counters\n\ninfo\nbean metrics:name=spark.0.executor.deserializeCpuTime,type=counters\n\ninfo\nbean metrics:name=spark.0.executor.deserializeTime,type=counters\n\ninfo\nbean metrics:name=spark.0.executor.diskBytesSpilled,type=counters\n\ninfo\nbean metrics:name=spark.0.executor.filesystem.file.largeRead_ops,type=gauges\n\ninfo\nbean metrics:name=spark.0.executor.filesystem.file.read_bytes,type=gauges\n\ninfo\nbean metrics:name=spark.0.executor.filesystem.file.read_ops,type=gauges\n\ninfo\nbean metrics:name=spark.0.executor.filesystem.file.write_bytes,type=gauges\n\ninfo\nbean metrics:name=spark.0.executor.filesystem.file.write_ops,type=gauges\n\ninfo\nbean metrics:name=spark.0.executor.filesystem.hdfs.largeRead_ops,type=gauges\n\ninfo\nbean metrics:name=spark.0.executor.filesystem.hdfs.read_bytes,type=gauges\n\ninfo\nbean metrics:name=spark.0.executor.filesystem.hdfs.read_ops,type=gauges\n\ninfo\nbean metrics:name=spark.0.executor.filesystem.hdfs.write_bytes,type=gauges\n\ninfo\nbean metrics:name=spark.0.executor.filesystem.hdfs.write_ops,type=gauges\n\ninfo\nbean metrics:name=spark.0.executor.jvmGCTime,type=counters\n\ninfo\nbean metrics:name=spark.0.executor.memoryBytesSpilled,type=counters\n\ninfo\nbean metrics:name=spark.0.executor.recordsRead,type=counters\n\ninfo\nbean metrics:name=spark.0.executor.recordsWritten,type=counters\n\ninfo\nbean metrics:name=spark.0.executor.resultSerializationTime,type=counters\n\ninfo\nbean metrics:name=spark.0.executor.resultSize,type=counters\n\ninfo\nbean metrics:name=spark.0.executor.runTime,type=counters\n\ninfo\nbean metrics:name=spark.0.executor.shuffleBytesWritten,type=counters\n\ninfo\nbean metrics:name=spark.0.executor.shuffleFetchWaitTime,type=counters\n\ninfo\nbean metrics:name=spark.0.executor.shuffleLocalBlocksFetched,type=counters\n\ninfo\nbean metrics:name=spark.0.executor.shuffleLocalBytesRead,type=counters\n\ninfo\nbean metrics:name=spark.0.executor.shuffleRecordsRead,type=counters\n\ninfo\nbean metrics:name=spark.0.executor.shuffleRecordsWritten,type=counters\n\ninfo\nbean metrics:name=spark.0.executor.shuffleRemoteBlocksFetched,type=counters\n\ninfo\nbean metrics:name=spark.0.executor.shuffleRemoteBytesRead,type=counters\n\ninfo\nbean metrics:name=spark.0.executor.shuffleRemoteBytesReadToDisk,type=counters\n\ninfo\nbean metrics:name=spark.0.executor.shuffleTotalBytesRead,type=counters\n\ninfo\nbean metrics:name=spark.0.executor.shuffleWriteTime,type=counters\n\ninfo\nbean metrics:name=spark.0.executor.succeededTasks,type=counters\n\ninfo\nbean metrics:name=spark.0.executor.threadpool.activeTasks,type=gauges\n\ninfo\nbean metrics:name=spark.0.executor.threadpool.completeTasks,type=gauges\n\ninfo\nbean metrics:name=spark.0.executor.threadpool.currentPool_size,type=gauges\n\ninfo\nbean metrics:name=spark.0.executor.threadpool.maxPool_size,type=gauges\n\ninfo\nbean metrics:name=spark.0.executor.threadpool.startedTasks,type=gauges\n\ninfo\n), 1 -&gt; Success(PID: 646\nJMImplementation\ncom.sun.management\njava.lang\njava.nio\njava.util.logging\njdk.management.jfr\norg.apache.logging.log4j2\nPID: 833\nJMImplementation\ncom.amazonaws.management\ncom.sun.management\njava.lang\njava.nio\njava.util.logging\njdk.management.jfr\nmetrics\norg.apache.logging.log4j2\nopen 833\nbean java.lang:name=Code\\ Cache,type=MemoryPool\n\ninfo\nbean java.lang:name=CodeCacheManager,type=MemoryManager\n\ninfo\nbean java.lang:name=Compressed\\ Class\\ Space,type=MemoryPool\n\ninfo\nbean java.lang:name=Metaspace\\ Manager,type=MemoryManager\n\ninfo\nbean java.lang:name=Metaspace,type=MemoryPool\n\ninfo\nbean java.lang:name=PS\\ Eden\\ Space,type=MemoryPool\n\ninfo\nbean java.lang:name=PS\\ MarkSweep,type=GarbageCollector\n\ninfo\nbean java.lang:name=PS\\ Old\\ Gen,type=MemoryPool\n\ninfo\nbean java.lang:name=PS\\ Scavenge,type=GarbageCollector\n\ninfo\nbean java.lang:name=PS\\ Survivor\\ Space,type=MemoryPool\n\ninfo\nbean java.lang:type=ClassLoading\n\ninfo\nbean java.lang:type=Compilation\n\ninfo\nbean java.lang:type=Memory\n\ninfo\nbean java.lang:type=OperatingSystem\n\ninfo\nbean java.lang:type=Runtime\n\ninfo\nbean java.lang:type=Threading\n\ninfo\nbean metrics:name=spark.1.CodeGenerator.compilationTime,type=histograms\n\ninfo\nbean metrics:name=spark.1.CodeGenerator.generatedClassSize,type=histograms\n\ninfo\nbean metrics:name=spark.1.CodeGenerator.generatedMethodSize,type=histograms\n\ninfo\nbean metrics:name=spark.1.CodeGenerator.sourceCodeSize,type=histograms\n\ninfo\nbean metrics:name=spark.1.Databricks.directoryCommit.autoVacuumCount,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.directoryCommit.deletedFilesFiltered,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.directoryCommit.filterListingCount,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.directoryCommit.jobCommitCompleted,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.directoryCommit.markerReadErrors,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.directoryCommit.markerRefreshCount,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.directoryCommit.markerRefreshErrors,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.directoryCommit.markersRead,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.directoryCommit.repeatedListCount,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.directoryCommit.uncommittedFilesFiltered,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.directoryCommit.untrackedFilesFound,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.directoryCommit.vacuumCount,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.directoryCommit.vacuumErrors,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.preemption.numChecks,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.preemption.numPoolsAutoExpired,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.preemption.numTasksPreempted,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.preemption.poolStarvationMillis,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.preemption.schedulerOverheadNanos,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.preemption.taskTimeWastedMillis,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.taskSchedulingLanes.activePools,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.taskSchedulingLanes.bypassLaneActivePools,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.taskSchedulingLanes.fastLaneActivePools,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.taskSchedulingLanes.finishedQueriesTotalTaskTimeNs,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.taskSchedulingLanes.laneCleanup.markedPools,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.taskSchedulingLanes.laneCleanup.twoPhasePoolsCleaned,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.taskSchedulingLanes.laneCleanup.zombiePoolsCleaned,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.taskSchedulingLanes.preemption.slotTransferNumSuccessfulPreemptionIterations,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.taskSchedulingLanes.preemption.slotTransferNumTasksPreempted,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.taskSchedulingLanes.preemption.slotTransferWastedTaskTimeNs,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.taskSchedulingLanes.slotReservation.numGradualDecrease,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.taskSchedulingLanes.slotReservation.numQuickDrop,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.taskSchedulingLanes.slotReservation.numQuickJump,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.taskSchedulingLanes.slotReservation.slotsReserved,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.taskSchedulingLanes.slowLaneActivePools,type=counters\n\ninfo\nbean metrics:name=spark.1.Databricks.taskSchedulingLanes.totalQueryGroupsFinished,type=counters\n\ninfo\nbean metrics:name=spark.1.ExecutorMetrics.DirectPoolMemory,type=gauges\n\ninfo\nbean metrics:name=spark.1.ExecutorMetrics.JVMHeapMemory,type=gauges\n\ninfo\nbean metrics:name=spark.1.ExecutorMetrics.JVMOffHeapMemory,type=gauges\n\ninfo\nbean metrics:name=spark.1.ExecutorMetrics.MajorGCCount,type=gauges\n\ninfo\nbean metrics:name=spark.1.ExecutorMetrics.MajorGCTime,type=gauges\n\ninfo\nbean metrics:name=spark.1.ExecutorMetrics.MappedPoolMemory,type=gauges\n\ninfo\nbean metrics:name=spark.1.ExecutorMetrics.MinorGCCount,type=gauges\n\ninfo\nbean metrics:name=spark.1.ExecutorMetrics.MinorGCTime,type=gauges\n\ninfo\nbean metrics:name=spark.1.ExecutorMetrics.OffHeapExecutionMemory,type=gauges\n\ninfo\nbean metrics:name=spark.1.ExecutorMetrics.OffHeapStorageMemory,type=gauges\n\ninfo\nbean metrics:name=spark.1.ExecutorMetrics.OffHeapUnifiedMemory,type=gauges\n\ninfo\nbean metrics:name=spark.1.ExecutorMetrics.OnHeapExecutionMemory,type=gauges\n\ninfo\nbean metrics:name=spark.1.ExecutorMetrics.OnHeapStorageMemory,type=gauges\n\ninfo\nbean metrics:name=spark.1.ExecutorMetrics.OnHeapUnifiedMemory,type=gauges\n\ninfo\nbean metrics:name=spark.1.ExecutorMetrics.ProcessTreeJVMRSSMemory,type=gauges\n\ninfo\nbean metrics:name=spark.1.ExecutorMetrics.ProcessTreeJVMVMemory,type=gauges\n\ninfo\nbean metrics:name=spark.1.ExecutorMetrics.ProcessTreeOtherRSSMemory,type=gauges\n\ninfo\nbean metrics:name=spark.1.ExecutorMetrics.ProcessTreeOtherVMemory,type=gauges\n\ninfo\nbean metrics:name=spark.1.ExecutorMetrics.ProcessTreePythonRSSMemory,type=gauges\n\ninfo\nbean metrics:name=spark.1.ExecutorMetrics.ProcessTreePythonVMemory,type=gauges\n\ninfo\nbean metrics:name=spark.1.ExecutorMetrics.TotalGCTime,type=gauges\n\ninfo\nbean metrics:name=spark.1.ExternalShuffle.shuffle-client.usedDirectMemory,type=gauges\n\ninfo\nbean metrics:name=spark.1.ExternalShuffle.shuffle-client.usedHeapMemory,type=gauges\n\ninfo\nbean metrics:name=spark.1.HiveExternalCatalog.fileCacheHits,type=counters\n\ninfo\nbean metrics:name=spark.1.HiveExternalCatalog.filesDiscovered,type=counters\n\ninfo\nbean metrics:name=spark.1.HiveExternalCatalog.hiveClientCalls,type=counters\n\ninfo\nbean metrics:name=spark.1.HiveExternalCatalog.parallelListingJobCount,type=counters\n\ninfo\nbean metrics:name=spark.1.HiveExternalCatalog.partitionsFetched,type=counters\n\ninfo\nbean metrics:name=spark.1.JVMCPU.jvmCpuTime,type=gauges\n\ninfo\nbean metrics:name=spark.1.executor.bytesRead,type=counters\n\ninfo\nbean metrics:name=spark.1.executor.bytesWritten,type=counters\n\ninfo\nbean metrics:name=spark.1.executor.cpuTime,type=counters\n\ninfo\nbean metrics:name=spark.1.executor.deserializeCpuTime,type=counters\n\ninfo\nbean metrics:name=spark.1.executor.deserializeTime,type=counters\n\ninfo\nbean metrics:name=spark.1.executor.diskBytesSpilled,type=counters\n\ninfo\nbean metrics:name=spark.1.executor.filesystem.file.largeRead_ops,type=gauges\n\ninfo\nbean metrics:name=spark.1.executor.filesystem.file.read_bytes,type=gauges\n\ninfo\nbean metrics:name=spark.1.executor.filesystem.file.read_ops,type=gauges\n\ninfo\nbean metrics:name=spark.1.executor.filesystem.file.write_bytes,type=gauges\n\ninfo\nbean metrics:name=spark.1.executor.filesystem.file.write_ops,type=gauges\n\ninfo\nbean metrics:name=spark.1.executor.filesystem.hdfs.largeRead_ops,type=gauges\n\ninfo\nbean metrics:name=spark.1.executor.filesystem.hdfs.read_bytes,type=gauges\n\ninfo\nbean metrics:name=spark.1.executor.filesystem.hdfs.read_ops,type=gauges\n\ninfo\nbean metrics:name=spark.1.executor.filesystem.hdfs.write_bytes,type=gauges\n\ninfo\nbean metrics:name=spark.1.executor.filesystem.hdfs.write_ops,type=gauges\n\ninfo\nbean metrics:name=spark.1.executor.jvmGCTime,type=counters\n\ninfo\nbean metrics:name=spark.1.executor.memoryBytesSpilled,type=counters\n\ninfo\nbean metrics:name=spark.1.executor.recordsRead,type=counters\n\ninfo\nbean metrics:name=spark.1.executor.recordsWritten,type=counters\n\ninfo\nbean metrics:name=spark.1.executor.resultSerializationTime,type=counters\n\ninfo\nbean metrics:name=spark.1.executor.resultSize,type=counters\n\ninfo\nbean metrics:name=spark.1.executor.runTime,type=counters\n\ninfo\nbean metrics:name=spark.1.executor.shuffleBytesWritten,type=counters\n\ninfo\nbean metrics:name=spark.1.executor.shuffleFetchWaitTime,type=counters\n\ninfo\nbean metrics:name=spark.1.executor.shuffleLocalBlocksFetched,type=counters\n\ninfo\nbean metrics:name=spark.1.executor.shuffleLocalBytesRead,type=counters\n\ninfo\nbean metrics:name=spark.1.executor.shuffleRecordsRead,type=counters\n\ninfo\nbean metrics:name=spark.1.executor.shuffleRecordsWritten,type=counters\n\ninfo\nbean metrics:name=spark.1.executor.shuffleRemoteBlocksFetched,type=counters\n\ninfo\nbean metrics:name=spark.1.executor.shuffleRemoteBytesRead,type=counters\n\ninfo\nbean metrics:name=spark.1.executor.shuffleRemoteBytesReadToDisk,type=counters\n\ninfo\nbean metrics:name=spark.1.executor.shuffleTotalBytesRead,type=counters\n\ninfo\nbean metrics:name=spark.1.executor.shuffleWriteTime,type=counters\n\ninfo\nbean metrics:name=spark.1.executor.succeededTasks,type=counters\n\ninfo\nbean metrics:name=spark.1.executor.threadpool.activeTasks,type=gauges\n\ninfo\nbean metrics:name=spark.1.executor.threadpool.completeTasks,type=gauges\n\ninfo\nbean metrics:name=spark.1.executor.threadpool.currentPool_size,type=gauges\n\ninfo\nbean metrics:name=spark.1.executor.threadpool.maxPool_size,type=gauges\n\ninfo\nbean metrics:name=spark.1.executor.threadpool.startedTasks,type=gauges\n\ninfo\n))\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "// Run the script for each executor\n",
    "\n",
    "import scala.concurrent.duration._\n",
    "\n",
    "var res=sc.runOnEachExecutor[String]({ () =>\n",
    "import sys.process._\n",
    "  \n",
    "       var cmd_Result=Seq(\"bash\", \"-c\", \"/dbfs/metrics/dump-jmx-metrics.sh\").!!\n",
    "      cmd_Result\n",
    "    }, 100.seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eff8e28-0192-443c-99e6-1f2711aabc6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[0m\u001B[01;32m10.139.64.4-833.txt\u001B[0m*  \u001B[01;32m10.139.64.7-833.txt\u001B[0m*  \u001B[01;32mdriver-10.139.64.8-813.txt\u001B[0m*\r\n"
     ]
    }
   ],
   "source": [
    "%ls /dbfs/metrics/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "222ad545-ce29-4d95-a14a-0eb650a296be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parse the detailed output of jmxterm (text file with mbean names and attributes)\n",
    "# into the JSON format of \"jmxMetrics\" field of the applicationinsights.json file.\n",
    "\n",
    "import os, re, json\n",
    "\n",
    "# Define the directory path where the jmxterm output files are located.\n",
    "# Output JSON files are also written in this location.\n",
    "directory = '/dbfs/metrics/result'\n",
    "\n",
    "# Read each .txt file in the directory.\n",
    "for filename in os.listdir(directory):\n",
    "    if not filename.endswith('.txt'):\n",
    "        continue\n",
    "\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    \n",
    "    # read the file\n",
    "    with open(filepath, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # read all mbean/attribute combinations into a results list\n",
    "    results = []\n",
    "    in_attributes_section = False\n",
    "    for line in lines:\n",
    "        if line.startswith('#'):\n",
    "            in_attributes_section = line.startswith('# attributes')\n",
    "            if line.startswith('#mbean = '):\n",
    "                mbean_name = line.rstrip().removeprefix('#mbean = ')\n",
    "                mbean_name_short = mbean_name.split(',')[0]\n",
    "        elif in_attributes_section:\n",
    "            match = re.search(r'  - (\\S*).*,', line) \n",
    "            result_string = match.group(1)\n",
    "            results.append({\"name\": mbean_name_short, \"objectName\": mbean_name, \"attribute\": result_string})\n",
    "\n",
    "    # filter and process results\n",
    "    filtered_results = []\n",
    "\n",
    "    for result in results:\n",
    "        others = len([1 for r in results if r[\"objectName\"] == result[\"objectName\"]])\n",
    "        if others > 1:\n",
    "            # Add the attribute name to the metric name, except when there is only one attribute\n",
    "            # (or the other attribute is \"Value\" and this attribute is \"Number\", see exclusion rule below)\n",
    "            if not(result[\"attribute\"] == \"Number\" and not any(r[\"objectName\"] == result[\"objectName\"] and r[\"attribute\"] != \"Value\" and r[\"attribute\"] != \"Number\" for r in results)):\n",
    "                result[\"name\"] = f'{result[\"name\"]}.{result[\"attribute\"]}' \n",
    "\n",
    "        # Make Spark metric names shorter and independent of the executor ID\n",
    "        result[\"name\"] = re.sub(r'^metrics:name=spark\\.\\d+\\.', 'spark.worker.', result[\"name\"])\n",
    "        result[\"name\"] = re.sub(r'^metrics:name=spark\\.', 'spark.', result[\"name\"])\n",
    "\n",
    "        # Exclude weird object names from nested/anonymous classes\n",
    "        if '$' in result[\"objectName\"]:\n",
    "            continue\n",
    "\n",
    "        # Exclude \"Value\" attributes when a \"Number\" attribute exists, since their values are then identical\n",
    "        if result[\"attribute\"] == \"Value\" and any(r[\"objectName\"] == result[\"objectName\"] and r[\"attribute\"] == \"Number\" for r in results):\n",
    "            continue\n",
    "\n",
    "        filtered_results.append(result)\n",
    "    \n",
    "    # sort and output results\n",
    "    sorted_results = sorted(filtered_results, key=lambda x: (x['name'], x['objectName'], x['attribute']))\n",
    "        \n",
    "    with open(filepath + '.json', 'w') as f:\n",
    "        json.dump(filtered_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86d4c9ef-40bd-4b99-b920-32c87c4b2d0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dbfs/metrics/result/10.139.64.4-833.txt.json:        \"name\": \"spark.worker.executor.threadpool.activeTasks\",\n/dbfs/metrics/result/10.139.64.4-833.txt.json:        \"objectName\": \"metrics:name=spark.1.executor.threadpool.activeTasks,type=gauges\",\n/dbfs/metrics/result/10.139.64.7-833.txt.json:        \"name\": \"spark.worker.executor.threadpool.activeTasks\",\n/dbfs/metrics/result/10.139.64.7-833.txt.json:        \"objectName\": \"metrics:name=spark.0.executor.threadpool.activeTasks,type=gauges\",\n/dbfs/metrics/result/10.139.64.4-833.txt.json:        \"name\": \"java.lang:type=OperatingSystem.ProcessCpuLoad\",\n/dbfs/metrics/result/10.139.64.4-833.txt.json:        \"attribute\": \"ProcessCpuLoad\"\n/dbfs/metrics/result/10.139.64.7-833.txt.json:        \"name\": \"java.lang:type=OperatingSystem.ProcessCpuLoad\",\n/dbfs/metrics/result/10.139.64.7-833.txt.json:        \"attribute\": \"ProcessCpuLoad\"\n/dbfs/metrics/result/driver-10.139.64.8-813.txt.json:        \"name\": \"java.lang:type=OperatingSystem.ProcessCpuLoad\",\n/dbfs/metrics/result/driver-10.139.64.8-813.txt.json:        \"attribute\": \"ProcessCpuLoad\"\n/dbfs/metrics/result/10.139.64.4-833.txt.json:        \"name\": \"spark.worker.ExecutorMetrics.OnHeapStorageMemory\",\n/dbfs/metrics/result/10.139.64.4-833.txt.json:        \"objectName\": \"metrics:name=spark.1.ExecutorMetrics.OnHeapStorageMemory,type=gauges\",\n/dbfs/metrics/result/10.139.64.7-833.txt.json:        \"name\": \"spark.worker.ExecutorMetrics.OnHeapStorageMemory\",\n/dbfs/metrics/result/10.139.64.7-833.txt.json:        \"objectName\": \"metrics:name=spark.0.ExecutorMetrics.OnHeapStorageMemory,type=gauges\",\n/dbfs/metrics/result/driver-10.139.64.8-813.txt.json:        \"name\": \"spark.driver.ExecutorMetrics.OnHeapStorageMemory\",\n/dbfs/metrics/result/driver-10.139.64.8-813.txt.json:        \"objectName\": \"metrics:name=spark.driver.ExecutorMetrics.OnHeapStorageMemory,type=gauges\",\n/dbfs/metrics/result/10.139.64.4-833.txt.json:        \"name\": \"java.lang:name=Code Cache.CollectionUsageThresholdExceeded\",\n/dbfs/metrics/result/10.139.64.4-833.txt.json:        \"attribute\": \"CollectionUsageThresholdExceeded\"\n/dbfs/metrics/result/10.139.64.4-833.txt.json:        \"name\": \"java.lang:name=Compressed Class Space.CollectionUsageThresholdExceeded\",\n/dbfs/metrics/result/10.139.64.4-833.txt.json:        \"attribute\": \"CollectionUsageThresholdExceeded\"\n/dbfs/metrics/result/10.139.64.4-833.txt.json:        \"name\": \"java.lang:name=Metaspace.CollectionUsageThresholdExceeded\",\n/dbfs/metrics/result/10.139.64.4-833.txt.json:        \"attribute\": \"CollectionUsageThresholdExceeded\"\n/dbfs/metrics/result/10.139.64.4-833.txt.json:        \"name\": \"java.lang:name=PS Eden Space.CollectionUsageThresholdExceeded\",\n/dbfs/metrics/result/10.139.64.4-833.txt.json:        \"attribute\": \"CollectionUsageThresholdExceeded\"\n/dbfs/metrics/result/10.139.64.4-833.txt.json:        \"name\": \"java.lang:name=PS Old Gen.CollectionUsageThresholdExceeded\",\n/dbfs/metrics/result/10.139.64.4-833.txt.json:        \"attribute\": \"CollectionUsageThresholdExceeded\"\n/dbfs/metrics/result/10.139.64.4-833.txt.json:        \"name\": \"java.lang:name=PS Survivor Space.CollectionUsageThresholdExceeded\",\n/dbfs/metrics/result/10.139.64.4-833.txt.json:        \"attribute\": \"CollectionUsageThresholdExceeded\"\n/dbfs/metrics/result/10.139.64.7-833.txt.json:        \"name\": \"java.lang:name=Code Cache.CollectionUsageThresholdExceeded\",\n/dbfs/metrics/result/10.139.64.7-833.txt.json:        \"attribute\": \"CollectionUsageThresholdExceeded\"\n/dbfs/metrics/result/10.139.64.7-833.txt.json:        \"name\": \"java.lang:name=Compressed Class Space.CollectionUsageThresholdExceeded\",\n/dbfs/metrics/result/10.139.64.7-833.txt.json:        \"attribute\": \"CollectionUsageThresholdExceeded\"\n/dbfs/metrics/result/10.139.64.7-833.txt.json:        \"name\": \"java.lang:name=Metaspace.CollectionUsageThresholdExceeded\",\n/dbfs/metrics/result/10.139.64.7-833.txt.json:        \"attribute\": \"CollectionUsageThresholdExceeded\"\n/dbfs/metrics/result/10.139.64.7-833.txt.json:        \"name\": \"java.lang:name=PS Eden Space.CollectionUsageThresholdExceeded\",\n/dbfs/metrics/result/10.139.64.7-833.txt.json:        \"attribute\": \"CollectionUsageThresholdExceeded\"\n/dbfs/metrics/result/10.139.64.7-833.txt.json:        \"name\": \"java.lang:name=PS Old Gen.CollectionUsageThresholdExceeded\",\n/dbfs/metrics/result/10.139.64.7-833.txt.json:        \"attribute\": \"CollectionUsageThresholdExceeded\"\n/dbfs/metrics/result/10.139.64.7-833.txt.json:        \"name\": \"java.lang:name=PS Survivor Space.CollectionUsageThresholdExceeded\",\n/dbfs/metrics/result/10.139.64.7-833.txt.json:        \"attribute\": \"CollectionUsageThresholdExceeded\"\n/dbfs/metrics/result/driver-10.139.64.8-813.txt.json:        \"name\": \"java.lang:name=Code Cache.CollectionUsageThresholdExceeded\",\n/dbfs/metrics/result/driver-10.139.64.8-813.txt.json:        \"attribute\": \"CollectionUsageThresholdExceeded\"\n/dbfs/metrics/result/driver-10.139.64.8-813.txt.json:        \"name\": \"java.lang:name=Compressed Class Space.CollectionUsageThresholdExceeded\",\n/dbfs/metrics/result/driver-10.139.64.8-813.txt.json:        \"attribute\": \"CollectionUsageThresholdExceeded\"\n/dbfs/metrics/result/driver-10.139.64.8-813.txt.json:        \"name\": \"java.lang:name=Metaspace.CollectionUsageThresholdExceeded\",\n/dbfs/metrics/result/driver-10.139.64.8-813.txt.json:        \"attribute\": \"CollectionUsageThresholdExceeded\"\n/dbfs/metrics/result/driver-10.139.64.8-813.txt.json:        \"name\": \"java.lang:name=PS Eden Space.CollectionUsageThresholdExceeded\",\n/dbfs/metrics/result/driver-10.139.64.8-813.txt.json:        \"attribute\": \"CollectionUsageThresholdExceeded\"\n/dbfs/metrics/result/driver-10.139.64.8-813.txt.json:        \"name\": \"java.lang:name=PS Old Gen.CollectionUsageThresholdExceeded\",\n/dbfs/metrics/result/driver-10.139.64.8-813.txt.json:        \"attribute\": \"CollectionUsageThresholdExceeded\"\n/dbfs/metrics/result/driver-10.139.64.8-813.txt.json:        \"name\": \"java.lang:name=PS Survivor Space.CollectionUsageThresholdExceeded\",\n/dbfs/metrics/result/driver-10.139.64.8-813.txt.json:        \"attribute\": \"CollectionUsageThresholdExceeded\"\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "# Show some sample lines\n",
    "grep activeTask /dbfs/metrics/result/*.json\n",
    "grep ProcessCpuLoad /dbfs/metrics/result/*.json\n",
    "grep OnHeapStorageMemory /dbfs/metrics/result/*.json\n",
    "grep CollectionUsageThresholdExceeded /dbfs/metrics/result/*.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61e8c45e-f2cf-48c2-8da4-c91327b450b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: dbfs/metrics/result/10.139.64.4-833.txt (deflated 94%)\n  adding: dbfs/metrics/result/10.139.64.4-833.txt.json (deflated 94%)\n  adding: dbfs/metrics/result/10.139.64.7-833.txt (deflated 94%)\n  adding: dbfs/metrics/result/10.139.64.7-833.txt.json (deflated 94%)\n  adding: dbfs/metrics/result/driver-10.139.64.8-813.txt (deflated 95%)\n  adding: dbfs/metrics/result/driver-10.139.64.8-813.txt.json (deflated 96%)\n"
     ]
    }
   ],
   "source": [
    "%sh \n",
    "zip /tmp/metrics.zip /dbfs/metrics/result/*\n",
    "mv /tmp/metrics.zip /dbfs/FileStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbfd3fef-04dd-4f13-a7ba-daabd6d9fd8e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "After running the notebook, download the template JSON files at [metrics.zip](/files/metrics.zip)."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1396149010086304,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "dump-jmx",
   "notebookOrigID": 1396149010086299,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
